# 数据预处理

### 读取原始数据集
```python
train_data_path = r'exp1_data\train_data.txt'

corpus = []
with open(train_data_path, 'r') as f:
    lines = f.readlines()
    for line in lines:
        corpus.append(json.loads(line)['raw'])
```
### 去除停用词和标点
使用sklearn内置的停用词和一些标点
```python
# 去除停用词和标点
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
puncs = [',','.','(',')',';',':','[',']','{','}']

corpus = []
for line in corpus_raw:
    filtered_line = []
    words = line.split(' ')
    for word in words:
        if word not in ENGLISH_STOP_WORDS:
            for p in puncs:
                word = word.strip(p)
            filtered_line.append(word)
    corpus.append(filtered_line)
```
# 2-gram分词
使用sklearn的分词工具
正则表达式r'\b\w+\b'匹配单词边界。

```python
bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
                                    token_pattern=r'\b\w+\b', min_df=1) # 会提取至少两个字母的单词
analyze = bigram_vectorizer.build_analyzer()
X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
```

得到的X_2是8000*372129的0-1矩阵。

# Tf-idf
tf-idf的思想是为了防止 “the”, “a”, “is” 等信息含量低的高频词影响频数更少但信息量更高的词。

Tf意为 **term-frequency**，tf-idf意为**term-frequency** times **inverse document-frequency**。
$$
tf-idf(t,d) = tf(td) \times idf(t)
$$

$$
idf(t) = log(\frac{1+n}{1+df(t)})+1,
\text{其中n是document数量。} df(t)\text{是包含单词t的文档数量。}
$$

sklearn中的TfidfTransformer 和 TfidfVectorizer实现时log内分子是n而不是n+1。smooth_idf控制是否平滑。

tf-idf结果归一化：

$$
v_{norm} = \frac{v}{\|v\|_2}
$$

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

transformer = TfidfTransformer(smooth_idf=False)
tfidf = transformer.fit_transform(X_2)
```
这样直接执行会报内存不够。
检查代码发现X_2转成了非稀疏矩阵，不要`toarray()`。去掉后很快出了结果。
![Alt text](image.png)



# 交叉验证
